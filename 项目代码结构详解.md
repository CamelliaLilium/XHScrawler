# MediaCrawler 项目代码结构详解

## 📁 项目整体架构

本项目是一个专门针对小红书（XHS）的爬虫系统，采用异步编程和模块化设计，支持多种登录方式和数据存储方案。

## 🏗️ 核心模块结构

### 1. **入口文件**

#### `main.py` - 主程序入口
```python
# 核心功能：
- 程序启动入口
- 参数解析和配置初始化
- 平台工厂模式实例化
- 异步爬虫任务启动
- 数据库连接管理
```

**主要函数**：
- `main()` - 主程序逻辑
- 异步上下文管理器处理数据库连接
- 只保留小红书平台相关代码

---

### 2. **配置管理模块**

#### `config/` 目录
- `__init__.py` - 配置模块初始化
- `base_config.py` - 基础配置文件
- `db_config.py` - 数据库配置

**`base_config.py` 核心配置项**：
```python
# 平台配置
PLATFORM = "xhs"                    # 固定为小红书
CRAWLER_TYPE = "creator"             # 爬取类型：search/detail/creator

# 登录配置
LOGIN_TYPE = "cookie"                # 登录方式：qrcode/phone/cookie
COOKIES = "..."                      # Cookie字符串
HEADLESS = False                     # 是否无头浏览器

# 爬取控制
CRAWLER_MAX_NOTES_COUNT = 10         # 最大帖子数量
CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = 20  # 单帖子评论数
ENABLE_GET_COMMENTS = True           # 是否爬取评论
ENABLE_GET_SUB_COMMENTS = True       # 是否爬取二级评论

# 用户ID列表
XHS_CREATOR_ID_LIST = [...]          # 目标用户ID列表

# 超时设置
BROWSER_LAUNCH_TIMEOUT = 120         # 浏览器启动超时（秒）
```

---

### 3. **媒体平台核心模块**

#### `media_platform/xhs/` - 小红书核心实现

##### `core.py` - 爬虫核心逻辑
**主要类**：`XiaoHongShuCrawler`

**核心方法**：
```python
async def start(self) -> None:
    """爬虫启动入口，根据配置选择爬取模式"""
    
async def search(self) -> None:
    """关键词搜索模式"""
    
async def get_specified_notes(self) -> None:
    """指定帖子详情爬取模式"""
    
async def get_creators_and_notes(self) -> None:
    """创作者主页爬取模式 - 核心功能"""
    # 1. 获取用户基本信息
    # 2. 爬取用户所有帖子
    # 3. 爬取每个帖子的评论
    
async def fetch_creator_notes_detail(self, note_list: List[Dict]):
    """批量处理创作者帖子详情"""
```

##### `client.py` - API客户端
**主要类**：`XiaoHongShuClient`

**核心API方法**：
```python
async def get_creator_info(self, user_id: str) -> Dict:
    """获取创作者基本信息"""
    
async def get_notes_by_creator(self, creator: str, cursor: str, page_size: int = 30) -> Dict:
    """分页获取创作者帖子列表"""
    
async def get_all_notes_by_creator(self, user_id: str, callback=None) -> List[Dict]:
    """获取创作者所有帖子（自动翻页）"""
    
async def get_note_by_id(self, note_id: str) -> Dict:
    """根据ID获取帖子详情"""
    
async def get_note_comments(self, note_id: str, cursor: str = "") -> Dict:
    """获取帖子评论列表"""
    
async def get_note_sub_comments(self, note_id: str, root_comment_id: str, num: int = 30, cursor: str = "") -> Dict:
    """获取二级评论（回复）"""
```

##### `login.py` - 登录管理
**主要功能**：
- Cookie登录验证
- 二维码登录处理
- 登录状态保持
- 反爬虫机制处理

##### `field.py` - 数据字段定义
**枚举类型**：
- 搜索排序类型
- 发布时间类型
- 数据字段映射

---

### 4. **数据存储模块**

#### `store/xhs/` - 小红书数据存储
**核心文件**：
- `xhs_store.py` - 数据保存逻辑

**主要函数**：
```python
async def save_creator(user_id: str, creator: Dict) -> None:
    """保存创作者信息到数据库"""
    
async def save_note(note_item: Dict) -> None:
    """保存帖子信息到数据库"""
    
async def save_comment(comment_item: Dict) -> None:
    """保存评论信息到数据库"""
    
async def save_sub_comment(comment_item: Dict) -> None:
    """保存二级评论到数据库"""
```

#### `db.py` - 数据库管理
**核心功能**：
- 数据库连接池管理
- 异步数据库操作
- 数据去重处理
- 事务管理

**主要函数**：
```python
async def init_db() -> None:
    """初始化数据库连接"""
    
async def close_db() -> None:
    """关闭数据库连接"""
    
class AsyncMysqlDB:
    """异步MySQL数据库操作类"""
    async def execute(self, sql: str, params: Optional[Tuple] = None) -> int
    async def query(self, sql: str, params: Optional[Tuple] = None) -> List[Dict]
```

---

### 5. **浏览器自动化模块**

#### `tools/` 目录

##### `browser_launcher.py` - 浏览器启动器
**功能**：
- 自动检测Chrome/Edge浏览器
- 启动浏览器进程
- CDP模式支持
- 浏览器状态监控

##### `cdp_browser.py` - CDP浏览器控制
**功能**：
- Chrome DevTools Protocol集成
- 反检测浏览器环境
- 用户数据目录管理

---

### 6. **数据分析模块**

#### 数据导出和分析脚本

##### `export_data.py` - 数据导出工具
```python
# 功能：
- 从数据库导出数据到CSV/JSON
- 数据格式化处理
- 批量导出支持
```

##### `comment_analyzer.py` - 评论分析工具
```python
# 功能：
- 评论数据统计分析
- 用户互动分析
- 时间趋势分析
```

##### `comment_tree_visualizer.py` - 评论树可视化
```python
# 功能：
- 生成评论回复关系树
- 可视化评论层级结构
- 支持多种输出格式
```

---

### 7. **数据模型**

#### `model/` 目录
- `m_xiaohongshu.py` - 小红书数据模型定义

**数据模型类**：
```python
class XhsCreator:
    """创作者信息模型"""
    
class XhsNote:
    """帖子信息模型"""
    
class XhsNoteComment:
    """评论信息模型"""
```

#### `schema/tables.sql` - 数据库表结构
**核心表**：
```sql
-- 创作者信息表
xhs_creator (user_id, nickname, avatar, desc, ...)

-- 帖子信息表  
xhs_note (note_id, title, desc, create_time, liked_count, ...)

-- 评论信息表
xhs_note_comment (comment_id, note_id, content, create_time, parent_comment_id, ...)
```

---

## 🔄 数据流程

### 1. **程序启动流程**
```
main.py 
  → 读取配置 
  → 初始化数据库 
  → 创建爬虫实例 
  → 启动爬取任务
```

### 2. **创作者爬取流程**
```
get_creators_and_notes()
  → 遍历 XHS_CREATOR_ID_LIST
  → get_creator_info() 获取用户信息
  → save_creator() 保存用户信息
  → get_all_notes_by_creator() 获取所有帖子
  → fetch_creator_notes_detail() 处理帖子详情
  → get_note_comments() 获取评论
  → get_note_sub_comments() 获取二级评论
  → 保存到数据库
```

### 3. **数据库操作流程**
```
数据获取 
  → 数据清洗和格式化 
  → 重复数据检查 
  → 异步写入数据库 
  → 事务提交
```

---

## 🛡️ 反爬虫机制

### 1. **请求控制**
- 随机延迟间隔
- 请求频率限制
- User-Agent轮换

### 2. **浏览器伪装**
- 真实浏览器环境
- Cookie状态保持
- 反检测JavaScript

### 3. **代理支持**
- IP代理池
- 代理轮换机制
- 失败重试

---

## 📊 数据处理特色

### 1. **评论关系处理**
- 支持多级评论结构
- `parent_comment_id` 字段维护回复关系
- 评论树可视化分析

### 2. **数据去重**
- 基于主键的数据去重
- 增量更新支持
- 数据完整性保证

### 3. **异步处理**
- 全异步IO操作
- 并发控制机制
- 资源优化管理

---

## 🎯 核心优势

1. **专一性**：专注小红书平台，功能深度优化
2. **可扩展性**：模块化设计，易于功能扩展
3. **稳定性**：完整的错误处理和重试机制
4. **数据完整性**：支持评论回复关系的完整爬取
5. **反检测能力**：多层反爬虫机制
6. **分析能力**：内置数据分析和可视化工具

这个项目通过精心设计的架构，实现了高效、稳定的小红书数据爬取和分析功能。
