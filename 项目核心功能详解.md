# MediaCrawler é¡¹ç›®æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

## ğŸ¯ é¡¹ç›®æ ¸å¿ƒæµç¨‹åˆ†æ

### 1. **ç¨‹åºå¯åŠ¨æµç¨‹è¯¦è§£**

#### `main.py` - ç¨‹åºå…¥å£ç‚¹
```python
async def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    await cmd_arg.parse_cmd()
    
    # 2. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ï¼ˆå¦‚æœé…ç½®ä¸ºæ•°æ®åº“å­˜å‚¨ï¼‰
    if config.SAVE_DATA_OPTION == "db":
        await db.init_db()
    
    # 3. åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆå·¥å‚æ¨¡å¼ï¼‰
    crawler = CrawlerFactory.create_crawler(platform=config.PLATFORM)
    
    # 4. å¯åŠ¨çˆ¬è™«ä»»åŠ¡
    await crawler.start()
    
    # 5. å…³é—­æ•°æ®åº“è¿æ¥
    if config.SAVE_DATA_OPTION == "db":
        await db.close()
```

**å…³é”®è®¾è®¡æ¨¡å¼**ï¼š
- **å·¥å‚æ¨¡å¼**ï¼š`CrawlerFactory` è´Ÿè´£æ ¹æ®å¹³å°åˆ›å»ºå¯¹åº”çš„çˆ¬è™«å®ä¾‹
- **å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šç¡®ä¿æ•°æ®åº“è¿æ¥çš„æ­£ç¡®åˆå§‹åŒ–å’Œæ¸…ç†

---

## ğŸ•·ï¸ çˆ¬è™«æ ¸å¿ƒå®ç°

### 2. **XiaoHongShuCrawler æ ¸å¿ƒç±»åˆ†æ**

#### ç±»åˆå§‹åŒ–å’Œæµè§ˆå™¨å¯åŠ¨
```python
class XiaoHongShuCrawler(AbstractCrawler):
    def __init__(self):
        self.index_url = "https://www.xiaohongshu.com"
        self.user_agent = config.UA or "é»˜è®¤UA"
        self.cdp_manager = None
```

#### å¯åŠ¨æµç¨‹ `start()` æ–¹æ³•
```python
async def start(self) -> None:
    # 1. ä»£ç†é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if config.ENABLE_IP_PROXY:
        ip_proxy_pool = await create_ip_pool(...)
        
    # 2. æµè§ˆå™¨å¯åŠ¨é€‰æ‹©
    if config.ENABLE_CDP_MODE:
        # CDPæ¨¡å¼ï¼šä½¿ç”¨ç°æœ‰æµè§ˆå™¨
        self.browser_context = await self.launch_browser_with_cdp(...)
    else:
        # æ ‡å‡†æ¨¡å¼ï¼šå¯åŠ¨æ–°æµè§ˆå™¨
        self.browser_context = await self.launch_browser(...)
    
    # 3. åæ£€æµ‹è®¾ç½®
    await self.browser_context.add_init_script(path="libs/stealth.min.js")
    await self.browser_context.add_cookies([...])  # æ·»åŠ webId cookie
    
    # 4. åˆ›å»ºå®¢æˆ·ç«¯
    self.xhs_client = await self.create_xhs_client(...)
    
    # 5. ç™»å½•æ£€æŸ¥å’Œå¤„ç†
    if not await self.xhs_client.pong():
        login_obj = XiaoHongShuLogin(...)
        await login_obj.begin()
        
    # 6. æ ¹æ®çˆ¬å–ç±»å‹æ‰§è¡Œä¸åŒé€»è¾‘
    if config.CRAWLER_TYPE == "creator":
        await self.get_creators_and_notes()
```

---

### 3. **åˆ›ä½œè€…çˆ¬å–æ ¸å¿ƒæµç¨‹**

#### `get_creators_and_notes()` è¯¦ç»†åˆ†æ
```python
async def get_creators_and_notes(self) -> None:
    for user_id in config.XHS_CREATOR_ID_LIST:
        # æ­¥éª¤1: è·å–åˆ›ä½œè€…åŸºæœ¬ä¿¡æ¯
        creator_info = await self.xhs_client.get_creator_info(user_id=user_id)
        if creator_info:
            await xhs_store.save_creator(user_id, creator=creator_info)
        
        # æ­¥éª¤2: è®¾ç½®çˆ¬å–é—´éš”ï¼ˆé˜²åçˆ¬ï¼‰
        if config.ENABLE_IP_PROXY:
            crawl_interval = random.random()
        else:
            crawl_interval = random.uniform(1, config.CRAWLER_MAX_SLEEP_SEC)
        
        # æ­¥éª¤3: è·å–åˆ›ä½œè€…æ‰€æœ‰å¸–å­
        all_notes_list = await self.xhs_client.get_all_notes_by_creator(
            user_id=user_id,
            crawl_interval=crawl_interval,
            callback=self.fetch_creator_notes_detail,  # å›è°ƒå¤„ç†å¸–å­è¯¦æƒ…
        )
        
        # æ­¥éª¤4: æ”¶é›†å¸–å­IDå’Œtokenï¼Œæ‰¹é‡è·å–è¯„è®º
        note_ids = [note_item.get("note_id") for note_item in all_notes_list]
        xsec_tokens = [note_item.get("xsec_token") for note_item in all_notes_list]
        await self.batch_get_note_comments(note_ids, xsec_tokens)
```

#### `fetch_creator_notes_detail()` å¹¶å‘å¤„ç†
```python
async def fetch_creator_notes_detail(self, note_list: List[Dict]):
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
    semaphore = asyncio.Semaphore(config.MAX_CONCURRENCY_NUM)
    
    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡åˆ—è¡¨
    task_list = [
        self.get_note_detail_async_task(
            note_id=post_item.get("note_id"),
            xsec_source=post_item.get("xsec_source"),
            xsec_token=post_item.get("xsec_token"),
            semaphore=semaphore,
        )
        for post_item in note_list
    ]
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    note_details = await asyncio.gather(*task_list)
    
    # ä¿å­˜å¸–å­è¯¦æƒ…åˆ°æ•°æ®åº“
    for note_detail in note_details:
        if note_detail:
            await xhs_store.update_xhs_note(note_detail)
```

---

## ğŸŒ APIå®¢æˆ·ç«¯å®ç°

### 4. **XiaoHongShuClient æ ¸å¿ƒåŠŸèƒ½**

#### è¯·æ±‚ç­¾åå’Œè®¤è¯
```python
class XiaoHongShuClient(AbstractApiClient):
    def __init__(self, timeout=30, proxies=None, *, headers, playwright_page):
        self._host = "https://edith.xiaohongshu.com"
        self.timeout = timeout
        self.proxies = proxies
        self.headers = headers
        self.playwright_page = playwright_page
        
    async def _pre_headers(self, uri: str, data: dict = None) -> Dict:
        """
        è¯·æ±‚å¤´é¢„å¤„ç†ï¼Œæ·»åŠ ç­¾åä¿¡æ¯
        """
        encrypt_params = await sign(
            uri, data, self.headers.get("Cookie"), self.playwright_page
        )
        local_storage = await self.playwright_page.evaluate("localStorage")
        signs = ["x-s", "x-t"]
        for sign in signs:
            if sign in encrypt_params:
                self.headers[sign] = encrypt_params[sign]
        return self.headers
```

#### åˆ›ä½œè€…ä¿¡æ¯è·å–
```python
async def get_creator_info(self, user_id: str) -> Dict:
    """
    è·å–åˆ›ä½œè€…åŸºæœ¬ä¿¡æ¯
    """
    uri = f"/api/sns/web/v1/user/{user_id}"
    res = await self.get(uri)
    return res.get("basic_info", {})
```

#### åˆ›ä½œè€…å¸–å­è·å–ï¼ˆåˆ†é¡µï¼‰
```python
async def get_notes_by_creator(self, creator: str, cursor: str, page_size: int = 30) -> Dict:
    """
    åˆ†é¡µè·å–åˆ›ä½œè€…å¸–å­åˆ—è¡¨
    """
    uri = "/api/sns/web/v1/user_posted"
    data = {
        "user_id": creator,
        "cursor": cursor,
        "num": page_size,
        "image_formats": ["jpg", "webp", "avif"]
    }
    return await self.get(uri, data)

async def get_all_notes_by_creator(self, user_id: str, crawl_interval: float = 1.0, callback=None, max_count: int = 35) -> List[Dict]:
    """
    è·å–åˆ›ä½œè€…æ‰€æœ‰å¸–å­ï¼ˆè‡ªåŠ¨ç¿»é¡µï¼‰
    """
    result = []
    notes_has_more = True
    notes_cursor = ""
    
    while notes_has_more and len(result) < max_count:
        # åˆ†é¡µè·å–å¸–å­
        notes_res = await self.get_notes_by_creator(user_id, notes_cursor)
        
        notes_has_more = notes_res.get("has_more", False)
        notes_cursor = notes_res.get("cursor", "")
        
        if "notes" not in notes_res:
            break
            
        notes = notes_res["notes"]
        
        # æ§åˆ¶è¿”å›æ•°é‡
        if len(result) + len(notes) > max_count:
            notes = notes[:max_count - len(result)]
            
        # å›è°ƒå¤„ç†ï¼ˆå¼‚æ­¥å¤„ç†å¸–å­è¯¦æƒ…ï¼‰
        if callback:
            await callback(notes)
            
        await asyncio.sleep(crawl_interval)  # é˜²åçˆ¬å»¶è¿Ÿ
        result.extend(notes)
    
    return result
```

---

### 5. **è¯„è®ºç³»ç»Ÿå®ç°**

#### ä¸€çº§è¯„è®ºè·å–
```python
async def get_note_comments(self, note_id: str, xsec_token: str, cursor: str = "") -> Dict:
    """
    è·å–ä¸€çº§è¯„è®ºçš„API
    """
    uri = "/api/sns/web/v2/comment/page"
    params = {
        "note_id": note_id,
        "cursor": cursor,
        "top_comment_id": "",
        "image_formats": "jpg,webp,avif",
        "xsec_token": xsec_token,
    }
    return await self.get(uri, params)
```

#### äºŒçº§è¯„è®ºï¼ˆå›å¤ï¼‰è·å–
```python
async def get_note_sub_comments(self, note_id: str, root_comment_id: str, xsec_token: str, num: int = 10, cursor: str = "") -> Dict:
    """
    è·å–æŒ‡å®šçˆ¶è¯„è®ºä¸‹çš„å­è¯„è®ºçš„API
    """
    uri = "/api/sns/web/v2/comment/sub/page"
    params = {
        "note_id": note_id,
        "root_comment_id": root_comment_id,
        "num": num,
        "cursor": cursor,
        "xsec_token": xsec_token,
    }
    return await self.get(uri, params)
```

#### å®Œæ•´è¯„è®ºæ ‘è·å–
```python
async def get_note_all_comments(self, note_id: str, xsec_token: str, crawl_interval: float = 1.0, callback=None, max_count: int = 10) -> List[Dict]:
    """
    è·å–æŒ‡å®šç¬”è®°ä¸‹çš„æ‰€æœ‰è¯„è®ºï¼ˆåŒ…æ‹¬äºŒçº§è¯„è®ºï¼‰
    """
    result = []
    comments_has_more = True
    comments_cursor = ""
    
    while comments_has_more and len(result) < max_count:
        # è·å–ä¸€çº§è¯„è®º
        comments_res = await self.get_note_comments(note_id, xsec_token, comments_cursor)
        
        comments_has_more = comments_res.get("has_more", False)
        comments_cursor = comments_res.get("cursor", "")
        comments = comments_res["comments"]
        
        # å›è°ƒå¤„ç†ä¸€çº§è¯„è®º
        if callback:
            await callback(note_id, comments)
            
        result.extend(comments)
        
        # è·å–æ‰€æœ‰äºŒçº§è¯„è®º
        sub_comments = await self.get_comments_all_sub_comments(
            comments=comments,
            xsec_token=xsec_token,
            crawl_interval=crawl_interval,
            callback=callback,
        )
        result.extend(sub_comments)
        
        await asyncio.sleep(crawl_interval)
    
    return result
```

---

## ğŸ’¾ æ•°æ®å­˜å‚¨ç³»ç»Ÿ

### 6. **æ•°æ®åº“æ“ä½œå±‚**

#### `db.py` - å¼‚æ­¥æ•°æ®åº“ç®¡ç†
```python
class AsyncMysqlDB:
    def __init__(self):
        self.pool: Optional[aiomysql.Pool] = None
        
    async def init_db(self) -> "AsyncMysqlDB":
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        self.pool = await aiomysql.create_pool(
            host=db_config.MYSQL_DB_HOST,
            port=db_config.MYSQL_DB_PORT,
            user=db_config.MYSQL_DB_USER,
            password=db_config.MYSQL_DB_PWD,
            db=db_config.MYSQL_DB_NAME,
            charset="utf8mb4",
            autocommit=False,
            maxsize=10,
            minsize=1,
        )
        return self
        
    async def execute(self, sql: str, params: Optional[Tuple] = None) -> int:
        """æ‰§è¡ŒSQLå‘½ä»¤ï¼ˆINSERT/UPDATE/DELETEï¼‰"""
        async with self.pool.acquire() as connection:
            async with connection.cursor() as cursor:
                await cursor.execute(sql, params)
                await connection.commit()
                return cursor.rowcount
                
    async def query(self, sql: str, params: Optional[Tuple] = None) -> List[Dict]:
        """æŸ¥è¯¢æ•°æ®"""
        async with self.pool.acquire() as connection:
            async with connection.cursor(aiomysql.DictCursor) as cursor:
                await cursor.execute(sql, params)
                return await cursor.fetchall()
```

#### `store/xhs/xhs_store.py` - å°çº¢ä¹¦æ•°æ®å­˜å‚¨
```python
async def save_creator(user_id: str, creator: Dict) -> None:
    """
    ä¿å­˜åˆ›ä½œè€…ä¿¡æ¯åˆ°æ•°æ®åº“
    """
    # æ•°æ®æ¸…æ´—å’Œè½¬æ¢
    creator_obj = XhsCreator(
        user_id=user_id,
        nickname=creator.get("nickname", ""),
        avatar=creator.get("avatar", ""),
        desc=creator.get("desc", ""),
        # ... å…¶ä»–å­—æ®µ
    )
    
    # å»é‡æ’å…¥
    sql = """
    INSERT INTO xhs_creator (user_id, nickname, avatar, desc, ...)
    VALUES (%s, %s, %s, %s, ...)
    ON DUPLICATE KEY UPDATE 
    nickname=VALUES(nickname), avatar=VALUES(avatar), ...
    """
    await db.execute(sql, creator_obj.to_tuple())

async def save_note(note_item: Dict) -> None:
    """
    ä¿å­˜å¸–å­ä¿¡æ¯åˆ°æ•°æ®åº“
    """
    note_obj = XhsNote(**note_item)
    sql = """
    INSERT INTO xhs_note (note_id, title, desc, create_time, ...)
    VALUES (%s, %s, %s, %s, ...)
    ON DUPLICATE KEY UPDATE ...
    """
    await db.execute(sql, note_obj.to_tuple())

async def save_comment(comment_item: Dict) -> None:
    """
    ä¿å­˜è¯„è®ºä¿¡æ¯åˆ°æ•°æ®åº“ï¼ˆæ”¯æŒäºŒçº§è¯„è®ºå…³ç³»ï¼‰
    """
    comment_obj = XhsNoteComment(
        comment_id=comment_item.get("id"),
        note_id=comment_item.get("note_id"),
        content=comment_item.get("content"),
        parent_comment_id=comment_item.get("parent_comment_id", ""),  # äºŒçº§è¯„è®ºå…³ç³»
        # ... å…¶ä»–å­—æ®µ
    )
    
    sql = """
    INSERT INTO xhs_note_comment (comment_id, note_id, content, parent_comment_id, ...)
    VALUES (%s, %s, %s, %s, ...)
    ON DUPLICATE KEY UPDATE ...
    """
    await db.execute(sql, comment_obj.to_tuple())
```

---

## ğŸ” ç™»å½•å’Œåæ£€æµ‹

### 7. **ç™»å½•ç³»ç»Ÿ**

#### `media_platform/xhs/login.py`
```python
class XiaoHongShuLogin:
    def __init__(self, login_type: str, login_phone: str, browser_context, context_page, cookie_str: str):
        self.login_type = login_type
        self.browser_context = browser_context
        self.context_page = context_page
        self.cookie_str = cookie_str
        
    async def begin(self):
        """å¼€å§‹ç™»å½•æµç¨‹"""
        if self.login_type == "qrcode":
            await self.login_by_qrcode()
        elif self.login_type == "phone":
            await self.login_by_mobile()
        elif self.login_type == "cookie":
            await self.login_by_cookies()
            
    async def login_by_cookies(self):
        """Cookieç™»å½•"""
        if not self.cookie_str:
            raise ValueError("Cookieå­—ç¬¦ä¸²ä¸èƒ½ä¸ºç©º")
            
        # è§£æå’Œè®¾ç½®cookies
        cookies = self.parse_cookie_string(self.cookie_str)
        await self.browser_context.add_cookies(cookies)
        
        # éªŒè¯ç™»å½•çŠ¶æ€
        await self.context_page.reload()
        if await self.check_login_state():
            utils.logger.info("Cookieç™»å½•æˆåŠŸ")
        else:
            raise LoginException("Cookieç™»å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ")
```

### 8. **åæ£€æµ‹æœºåˆ¶**

#### JavaScriptåæ£€æµ‹
```javascript
// libs/stealth.min.js - åæ£€æµ‹è„šæœ¬
// ä¸»è¦åŠŸèƒ½ï¼š
// 1. éšè—webdriverç‰¹å¾
// 2. ä¼ªé€ navigatorå±æ€§  
// 3. é˜²æ­¢è‡ªåŠ¨åŒ–æ£€æµ‹
// 4. æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
```

#### è¯·æ±‚ç­¾åæœºåˆ¶
```python
# media_platform/xhs/help.py
async def sign(uri: str, data: dict, cookie: str, page: Page) -> Dict:
    """
    å°çº¢ä¹¦è¯·æ±‚ç­¾åç®—æ³•
    é€šè¿‡æ‰§è¡ŒJavaScriptè·å–x-så’Œx-tç­¾åå‚æ•°
    """
    # æ³¨å…¥ç­¾åJavaScriptä»£ç 
    sign_js = """
    // å°çº¢ä¹¦ç­¾åç®—æ³•å®ç°
    window._webmsxyw = function(uri, data) {
        // å¤æ‚çš„ç­¾åè®¡ç®—é€»è¾‘
        return {
            "x-s": "ç­¾åå€¼",
            "x-t": "æ—¶é—´æˆ³"
        };
    };
    """
    
    await page.evaluate(sign_js)
    result = await page.evaluate(f"window._webmsxyw('{uri}', {json.dumps(data)})")
    return result
```

---

## ğŸ“Š æ•°æ®åˆ†æå·¥å…·

### 9. **è¯„è®ºæ ‘åˆ†æ**

#### `comment_tree_visualizer.py`
```python
def build_comment_tree(comments: List[Dict]) -> Dict:
    """
    æ„å»ºè¯„è®ºæ ‘ç»“æ„
    """
    tree = {}
    
    # ç¬¬ä¸€éï¼šå¤„ç†æ‰€æœ‰ä¸€çº§è¯„è®º
    for comment in comments:
        if not comment.get('parent_comment_id'):
            tree[comment['comment_id']] = {
                'comment': comment,
                'replies': {}
            }
    
    # ç¬¬äºŒéï¼šå¤„ç†æ‰€æœ‰äºŒçº§è¯„è®º
    for comment in comments:
        parent_id = comment.get('parent_comment_id')
        if parent_id and parent_id in tree:
            tree[parent_id]['replies'][comment['comment_id']] = {
                'comment': comment,
                'replies': {}
            }
    
    return tree

def visualize_comment_tree(tree: Dict) -> str:
    """
    å¯è§†åŒ–è¯„è®ºæ ‘ç»“æ„
    """
    result = []
    
    for comment_id, node in tree.items():
        comment = node['comment']
        result.append(f"ğŸ“ {comment['content'][:50]}...")
        result.append(f"   ğŸ‘¤ {comment['nickname']} | â° {comment['create_time']}")
        
        # æ˜¾ç¤ºå›å¤
        for reply_id, reply_node in node['replies'].items():
            reply = reply_node['comment']
            result.append(f"    â””â”€ ğŸ’¬ {reply['content'][:40]}...")
            result.append(f"       ğŸ‘¤ {reply['nickname']}")
        
        result.append("")
    
    return "\\n".join(result)
```

### 10. **æ•°æ®å¯¼å‡ºå·¥å…·**

#### `export_data.py`
```python
async def export_to_csv(table_name: str, output_file: str):
    """å¯¼å‡ºæ•°æ®åˆ°CSV"""
    sql = f"SELECT * FROM {table_name}"
    data = await db.query(sql)
    
    df = pd.DataFrame(data)
    df.to_csv(output_file, index=False, encoding='utf-8-sig')

async def export_to_json(table_name: str, output_file: str):
    """å¯¼å‡ºæ•°æ®åˆ°JSON"""
    sql = f"SELECT * FROM {table_name}"
    data = await db.query(sql)
    
    # å¤„ç†æ—¶é—´åºåˆ—åŒ–
    for row in data:
        for key, value in row.items():
            if isinstance(value, datetime):
                row[key] = value.strftime('%Y-%m-%d %H:%M:%S')
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
```

---

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿æ€»ç»“

### æŠ€æœ¯ç‰¹è‰²
1. **å…¨å¼‚æ­¥æ¶æ„**: åŸºäºasyncioå’Œaiohttpçš„é«˜æ€§èƒ½å¼‚æ­¥IO
2. **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„åˆ†å±‚æ¶æ„ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•
3. **æ™ºèƒ½åæ£€æµ‹**: å¤šå±‚åçˆ¬è™«æœºåˆ¶ï¼Œç¨³å®šæ€§å¼º
4. **å®Œæ•´æ•°æ®å…³ç³»**: æ”¯æŒè¯„è®ºå›å¤çš„å®Œæ•´å±‚çº§å…³ç³»
5. **ä¸°å¯Œåˆ†æå·¥å…·**: å†…ç½®æ•°æ®åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½

### åº”ç”¨åœºæ™¯
- ç”¨æˆ·è¡Œä¸ºåˆ†æ
- å†…å®¹è¶‹åŠ¿ç ”ç©¶  
- è¯„è®ºæƒ…æ„Ÿåˆ†æ
- ç¤¾äº¤ç½‘ç»œåˆ†æ
- ç«å“å†…å®¹ç›‘æ§

è¿™ä¸ªé¡¹ç›®é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„å’Œä¸°å¯Œçš„åŠŸèƒ½æ¨¡å—ï¼Œæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„å°çº¢ä¹¦æ•°æ®é‡‡é›†å’Œåˆ†æè§£å†³æ–¹æ¡ˆã€‚
